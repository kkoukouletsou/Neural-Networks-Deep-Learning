{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Import and Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from hyperopt import space_eval\n",
    "\n",
    "# Load Data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Name Classes\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Preprocess Data\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform Zero-Padding, Kernel and get Feature Map**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve(image, kernel, stride=1, padding=0):\n",
    "    _, _, channels = image.shape  \n",
    "    kernel_height, kernel_weight, _ = kernel.shape\n",
    "    padded_image = np.pad(image, ((padding, padding), (padding, padding), (0, 0)), mode='constant')\n",
    "    padded_height, padded_weight, _ = padded_image.shape\n",
    "\n",
    "    feature_map_height = (padded_height - kernel_height) // stride + 1 # floor division\n",
    "    feature_map_width = (padded_weight - kernel_weight) // stride + 1\n",
    "    feature_map = np.zeros((feature_map_height, feature_map_width))\n",
    "\n",
    "    # Apply the Kernel over the Image\n",
    "    for i in range(feature_map_height):\n",
    "        for j in range(feature_map_width):\n",
    "            region = padded_image[i * stride:i * stride + kernel_height, j * stride:j * stride + kernel_weight, :]\n",
    "            feature_map[i, j] = np.sum(region * kernel)\n",
    "\n",
    "    return feature_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activation Function -> ReLU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activation Function -> Softmax**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z))\n",
    "    return exp_z / np.sum(exp_z)\n",
    "\n",
    "def softmax_derivative(output, true_label):\n",
    "    return output - true_label  # Derivative for softmax + cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(grad_pooled, pool_size, original_shape):\n",
    "    \"\"\"\n",
    "    Upsample the gradient from the pooled feature map back to the original size.\n",
    "    \"\"\"\n",
    "    h, w = original_shape\n",
    "    upsampled = np.zeros(original_shape)\n",
    "    pooled_h, pooled_w = grad_pooled.shape\n",
    "\n",
    "    for i in range(pooled_h):\n",
    "        for j in range(pooled_w):\n",
    "            start_i, end_i = i * pool_size, (i + 1) * pool_size\n",
    "            start_j, end_j = j * pool_size, (j + 1) * pool_size\n",
    "            upsampled[start_i:end_i, start_j:end_j] = grad_pooled[i, j]\n",
    "\n",
    "    return upsampled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Max Pooling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pooling(feature_map, n):\n",
    "    rows, cols = feature_map.shape\n",
    "    new_rows = rows // n\n",
    "    new_cols = cols // n\n",
    "    new_array = np.zeros((new_rows, new_cols))\n",
    "\n",
    "    for i in range(new_rows):\n",
    "        for j in range(new_cols):\n",
    "            region = feature_map[i * n:(i + 1) * n, j * n:(j + 1) * n]\n",
    "            new_array[i, j] = np.max(region)\n",
    "\n",
    "    return new_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flattening Feature Map after MaxPooling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(feature_map):\n",
    "    return feature_map.flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dense Layer Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_layer(inputs, weights, bias, activation=None):\n",
    "    z = np.dot(weights, inputs) + bias\n",
    "    if activation == 'relu':\n",
    "        return relu(z)\n",
    "    elif activation == 'softmax':\n",
    "        return softmax(z)\n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_dense(dense_output, dense_weights, dense_bias,  prev_activation, true_label, activation='relu'):\n",
    "    grad_output = dense_output - true_label\n",
    "    grad_weights = np.outer(grad_output, prev_activation)\n",
    "    grad_bias = grad_output\n",
    "    grad_prev_activation = np.dot(dense_weights.T, grad_output)\n",
    "    return grad_weights, grad_bias, grad_prev_activation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_conv(feature_map, kernel, input_image, stride, padding):\n",
    "    kernel_h, kernel_w, kernel_c = kernel.shape\n",
    "    grad_kernel = np.zeros_like(kernel)\n",
    "    grad_input = np.zeros_like(input_image)\n",
    "\n",
    "    padded_input = np.pad(input_image, ((padding, padding), (padding, padding), (0, 0)), mode='constant')\n",
    "    grad_padded_input = np.pad(grad_input, ((padding, padding), (padding, padding), (0, 0)), mode='constant')\n",
    "\n",
    "    for i in range(feature_map.shape[0]):\n",
    "        for j in range(feature_map.shape[1]):\n",
    "            for k in range(kernel_c):\n",
    "                region = padded_input[i * stride:i * stride + kernel_h, j * stride:j * stride + kernel_w, k]\n",
    "                grad_kernel[:, :, k] += feature_map[i, j] * region\n",
    "                grad_padded_input[i * stride:i * stride + kernel_h, j * stride:j * stride + kernel_w, k] += kernel[:, :, k] * feature_map[i, j]\n",
    "\n",
    "    if padding > 0:\n",
    "        grad_input = grad_padded_input[padding:-padding, padding:-padding, :]\n",
    "    else:\n",
    "        grad_input = grad_padded_input\n",
    "\n",
    "    return grad_kernel, grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical Cross Entropy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def categorical_cross_entropy(y_true, y_pred):\n",
    "    epsilon = 1e-8\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.sum(y_true * np.log(y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.1457\n",
      "Epoch 2, Loss: 1.8736\n",
      "Epoch 3, Loss: 1.8042\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training Function\n",
    "def train_cnn(x_train, y_train, kernel, dense_weights, dense_bias, epochs, learning_rate, padding, pool_size):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i in range(len(x_train)):\n",
    "            image = x_train[i]\n",
    "            true_label = y_train[i]\n",
    "\n",
    "            conv_output = convolve(image, kernel, stride=1, padding=padding)\n",
    "            relu_output = relu(conv_output)\n",
    "            pooled_output = max_pooling(relu_output, pool_size)\n",
    "            flat_output = flatten(pooled_output)\n",
    "            dense_output = dense_layer(flat_output, dense_weights, dense_bias, activation='softmax')\n",
    "            loss = -np.sum(true_label * np.log(dense_output + 1e-8))\n",
    "            total_loss += loss\n",
    "    \n",
    "            grad_weights_dense, grad_bias_dense, grad_flat = backprop_dense(dense_output, dense_weights, dense_bias, flat_output, true_label)\n",
    "            grad_pooled = grad_flat.reshape(pooled_output.shape)\n",
    "\n",
    "            grad_upsampled = upsample(grad_pooled, pool_size, relu_output.shape)\n",
    "            grad_relu = relu_derivative(relu_output) * grad_upsampled\n",
    "            grad_kernel, _ = backprop_conv(grad_relu, kernel, image, stride=1, padding=padding)\n",
    "\n",
    "            dense_weights -= learning_rate * grad_weights_dense\n",
    "            dense_bias -= learning_rate * grad_bias_dense\n",
    "            kernel -= learning_rate * grad_kernel\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(x_train):.4f}\")\n",
    "\n",
    "kernel = np.random.randn(3, 3, 3) * 0.1  \n",
    "dense_weights = np.random.randn(10, (32 // 2) * (32 // 2)) * 0.1  \n",
    "dense_bias = np.random.randn(10) * 0.1\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 5\n",
    "learning_rate = 0.001\n",
    "padding = 1\n",
    "pool_size = 2\n",
    "\n",
    "# Train the CNN\n",
    "train_cnn(x_train, y_train, kernel, dense_weights, dense_bias, epochs, learning_rate, padding, pool_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
